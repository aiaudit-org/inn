{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from context_encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 16385     \n",
      "=================================================================\n",
      "Total params: 387,841\n",
      "Trainable params: 386,945\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 512)         66048     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 128)       589952    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 1)         0         \n",
      "=================================================================\n",
      "Total params: 824,705\n",
      "Trainable params: 823,873\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n",
      "Done with #  500\n",
      "Done with #  1000\n",
      "Done with #  1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oala/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.635824, acc: 64.84%] [G loss: 0.833931, mse: 0.834335]\n",
      "200 [D loss: 0.000018, acc: 100.00%] [G loss: 0.279276, mse: 0.263421]\n",
      "400 [D loss: 0.000004, acc: 100.00%] [G loss: 0.261433, mse: 0.245561]\n",
      "600 [D loss: 0.000016, acc: 100.00%] [G loss: 0.255586, mse: 0.239971]\n",
      "800 [D loss: 0.000064, acc: 100.00%] [G loss: 0.238180, mse: 0.222411]\n",
      "1000 [D loss: 0.000509, acc: 100.00%] [G loss: 0.230435, mse: 0.220159]\n",
      "1200 [D loss: 0.000006, acc: 100.00%] [G loss: 0.214236, mse: 0.198580]\n",
      "1400 [D loss: 0.001632, acc: 100.00%] [G loss: 0.212645, mse: 0.197092]\n",
      "1600 [D loss: 0.000028, acc: 100.00%] [G loss: 0.203353, mse: 0.187965]\n",
      "1800 [D loss: 0.000010, acc: 100.00%] [G loss: 0.192761, mse: 0.177354]\n",
      "2000 [D loss: 0.000004, acc: 100.00%] [G loss: 0.195547, mse: 0.180860]\n",
      "2200 [D loss: 0.000023, acc: 100.00%] [G loss: 0.183290, mse: 0.168173]\n",
      "2400 [D loss: 0.000005, acc: 100.00%] [G loss: 0.182736, mse: 0.167314]\n",
      "2600 [D loss: 0.000010, acc: 100.00%] [G loss: 0.173028, mse: 0.158083]\n",
      "2800 [D loss: 0.000009, acc: 100.00%] [G loss: 0.178526, mse: 0.163154]\n",
      "3000 [D loss: 0.000013, acc: 100.00%] [G loss: 0.169023, mse: 0.153801]\n",
      "3200 [D loss: 0.000004, acc: 100.00%] [G loss: 0.167825, mse: 0.152881]\n",
      "3400 [D loss: 0.000015, acc: 100.00%] [G loss: 0.165691, mse: 0.150121]\n",
      "3600 [D loss: 0.000019, acc: 100.00%] [G loss: 0.167624, mse: 0.152341]\n",
      "3800 [D loss: 0.000003, acc: 100.00%] [G loss: 0.160523, mse: 0.145594]\n",
      "4000 [D loss: 0.000003, acc: 100.00%] [G loss: 0.163729, mse: 0.148165]\n",
      "4200 [D loss: 0.000009, acc: 100.00%] [G loss: 0.154134, mse: 0.138294]\n",
      "4400 [D loss: 0.000004, acc: 100.00%] [G loss: 0.158620, mse: 0.142932]\n",
      "4600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.150266, mse: 0.134425]\n",
      "4800 [D loss: 0.000008, acc: 100.00%] [G loss: 0.155088, mse: 0.139320]\n",
      "5000 [D loss: 0.000001, acc: 100.00%] [G loss: 0.155600, mse: 0.139875]\n",
      "5200 [D loss: 0.000001, acc: 100.00%] [G loss: 0.160324, mse: 0.144369]\n",
      "5400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.152348, mse: 0.136455]\n",
      "5600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.154210, mse: 0.138360]\n",
      "5800 [D loss: 0.000001, acc: 100.00%] [G loss: 0.147756, mse: 0.131777]\n",
      "6000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.152834, mse: 0.136963]\n",
      "6200 [D loss: 0.000002, acc: 100.00%] [G loss: 0.142479, mse: 0.126565]\n",
      "6400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.148613, mse: 0.132628]\n",
      "6600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.148492, mse: 0.132537]\n",
      "6800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.143297, mse: 0.127306]\n",
      "7000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.144786, mse: 0.128797]\n",
      "7200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.141374, mse: 0.125462]\n",
      "7400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.146463, mse: 0.130476]\n",
      "7600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.139022, mse: 0.123032]\n",
      "7800 [D loss: 0.000335, acc: 100.00%] [G loss: 0.142929, mse: 0.126938]\n",
      "8000 [D loss: 0.000001, acc: 100.00%] [G loss: 0.142752, mse: 0.126760]\n",
      "8200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.140712, mse: 0.124814]\n",
      "8400 [D loss: 0.000001, acc: 100.00%] [G loss: 0.140741, mse: 0.124812]\n",
      "8600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.141724, mse: 0.125731]\n",
      "8800 [D loss: 0.000004, acc: 100.00%] [G loss: 0.141037, mse: 0.125044]\n",
      "9000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.141968, mse: 0.125975]\n",
      "9200 [D loss: 0.000001, acc: 100.00%] [G loss: 0.137942, mse: 0.121946]\n",
      "9400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.136660, mse: 0.120663]\n",
      "9600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.139402, mse: 0.123407]\n",
      "9800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.140383, mse: 0.124390]\n",
      "10000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.139455, mse: 0.123460]\n",
      "10200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.131917, mse: 0.115914]\n",
      "10400 [D loss: 0.414468, acc: 91.41%] [G loss: 0.133862, mse: 0.124850]\n",
      "10600 [D loss: 0.008911, acc: 100.00%] [G loss: 0.108246, mse: 0.096949]\n",
      "10800 [D loss: 0.000596, acc: 100.00%] [G loss: 0.108300, mse: 0.098638]\n",
      "11000 [D loss: 0.002137, acc: 100.00%] [G loss: 0.106108, mse: 0.098401]\n",
      "11200 [D loss: 0.001429, acc: 100.00%] [G loss: 0.108247, mse: 0.102361]\n",
      "11400 [D loss: 0.001680, acc: 100.00%] [G loss: 0.110212, mse: 0.102223]\n",
      "11600 [D loss: 0.003281, acc: 100.00%] [G loss: 0.103745, mse: 0.099437]\n",
      "11800 [D loss: 0.010482, acc: 100.00%] [G loss: 0.095445, mse: 0.087405]\n",
      "12000 [D loss: 0.000552, acc: 100.00%] [G loss: 0.101525, mse: 0.095069]\n",
      "12200 [D loss: 0.000497, acc: 100.00%] [G loss: 0.100951, mse: 0.093631]\n",
      "12400 [D loss: 0.000906, acc: 100.00%] [G loss: 0.097529, mse: 0.090964]\n",
      "12600 [D loss: 0.001209, acc: 100.00%] [G loss: 0.105005, mse: 0.098191]\n",
      "12800 [D loss: 0.000078, acc: 100.00%] [G loss: 0.092732, mse: 0.090194]\n",
      "13000 [D loss: 0.000048, acc: 100.00%] [G loss: 0.097593, mse: 0.095442]\n",
      "13200 [D loss: 0.000005, acc: 100.00%] [G loss: 0.101030, mse: 0.100039]\n",
      "13400 [D loss: 0.000008, acc: 100.00%] [G loss: 0.095671, mse: 0.093221]\n",
      "13600 [D loss: 0.000041, acc: 100.00%] [G loss: 0.092644, mse: 0.089169]\n",
      "13800 [D loss: 0.000039, acc: 100.00%] [G loss: 0.092312, mse: 0.090960]\n",
      "14000 [D loss: 0.000004, acc: 100.00%] [G loss: 0.092263, mse: 0.091276]\n",
      "14200 [D loss: 0.000032, acc: 100.00%] [G loss: 0.096622, mse: 0.093805]\n",
      "14400 [D loss: 0.000003, acc: 100.00%] [G loss: 0.088356, mse: 0.086414]\n",
      "14600 [D loss: 0.000013, acc: 100.00%] [G loss: 0.093115, mse: 0.092181]\n",
      "14800 [D loss: 0.000559, acc: 100.00%] [G loss: 0.085649, mse: 0.084080]\n",
      "15000 [D loss: 0.000126, acc: 100.00%] [G loss: 0.084731, mse: 0.084317]\n",
      "15200 [D loss: 0.000662, acc: 100.00%] [G loss: 0.083708, mse: 0.083217]\n",
      "15400 [D loss: 0.000016, acc: 100.00%] [G loss: 0.191099, mse: 0.175156]\n",
      "15600 [D loss: 0.002363, acc: 100.00%] [G loss: 0.091827, mse: 0.090217]\n",
      "15800 [D loss: 0.000018, acc: 100.00%] [G loss: 0.088387, mse: 0.087914]\n",
      "16000 [D loss: 0.000057, acc: 100.00%] [G loss: 0.088388, mse: 0.087749]\n",
      "16200 [D loss: 0.000018, acc: 100.00%] [G loss: 0.086215, mse: 0.084928]\n",
      "16400 [D loss: 0.000002, acc: 100.00%] [G loss: 0.090177, mse: 0.088993]\n",
      "16600 [D loss: 0.000004, acc: 100.00%] [G loss: 0.092110, mse: 0.091408]\n",
      "16800 [D loss: 0.000004, acc: 100.00%] [G loss: 0.086942, mse: 0.085732]\n",
      "17000 [D loss: 0.000031, acc: 100.00%] [G loss: 0.085857, mse: 0.085230]\n",
      "17200 [D loss: 0.000079, acc: 100.00%] [G loss: 0.089760, mse: 0.086463]\n",
      "17400 [D loss: 0.000020, acc: 100.00%] [G loss: 0.090174, mse: 0.088858]\n",
      "17600 [D loss: 0.000030, acc: 100.00%] [G loss: 0.087687, mse: 0.086399]\n",
      "17800 [D loss: 0.000093, acc: 100.00%] [G loss: 0.080664, mse: 0.079280]\n",
      "18000 [D loss: 0.000018, acc: 100.00%] [G loss: 0.083163, mse: 0.082134]\n",
      "18200 [D loss: 0.000003, acc: 100.00%] [G loss: 0.081335, mse: 0.080440]\n",
      "18400 [D loss: 0.000031, acc: 100.00%] [G loss: 0.080310, mse: 0.079110]\n",
      "18600 [D loss: 0.000129, acc: 100.00%] [G loss: 0.079349, mse: 0.078082]\n",
      "18800 [D loss: 0.000088, acc: 100.00%] [G loss: 0.083222, mse: 0.080313]\n",
      "19000 [D loss: 0.000017, acc: 100.00%] [G loss: 0.084261, mse: 0.083022]\n",
      "19200 [D loss: 0.000016, acc: 100.00%] [G loss: 0.081766, mse: 0.080576]\n",
      "19400 [D loss: 0.000024, acc: 100.00%] [G loss: 0.087794, mse: 0.087607]\n",
      "19600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.085092, mse: 0.083174]\n",
      "19800 [D loss: 0.000240, acc: 100.00%] [G loss: 0.084148, mse: 0.083678]\n",
      "20000 [D loss: 0.000022, acc: 100.00%] [G loss: 0.083694, mse: 0.082680]\n",
      "20200 [D loss: 0.000010, acc: 100.00%] [G loss: 0.079638, mse: 0.079318]\n",
      "20400 [D loss: 0.000250, acc: 100.00%] [G loss: 0.080898, mse: 0.080506]\n",
      "20600 [D loss: 0.000003, acc: 100.00%] [G loss: 0.088936, mse: 0.072890]\n",
      "20800 [D loss: 0.000001, acc: 100.00%] [G loss: 0.088997, mse: 0.072951]\n",
      "21000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.091264, mse: 0.075222]\n",
      "21200 [D loss: 0.000001, acc: 100.00%] [G loss: 0.087985, mse: 0.071939]\n",
      "21400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.094203, mse: 0.078163]\n",
      "21600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.088486, mse: 0.072440]\n",
      "21800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.083667, mse: 0.067616]\n",
      "22000 [D loss: 0.000001, acc: 100.00%] [G loss: 0.083446, mse: 0.067396]\n",
      "22200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.091547, mse: 0.075504]\n",
      "22400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.085448, mse: 0.069399]\n",
      "22600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.090610, mse: 0.074567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.091134, mse: 0.075091]\n",
      "23000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.081730, mse: 0.065678]\n",
      "23200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.164671, mse: 0.148702]\n",
      "23400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.085530, mse: 0.069482]\n",
      "23600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.083492, mse: 0.067441]\n",
      "23800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.081457, mse: 0.065404]\n",
      "24000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.095398, mse: 0.079360]\n",
      "24200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.082984, mse: 0.066932]\n",
      "24400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.087471, mse: 0.071424]\n",
      "24600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.085530, mse: 0.069482]\n",
      "24800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.080746, mse: 0.064692]\n",
      "25000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.086802, mse: 0.070755]\n",
      "25200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.083972, mse: 0.067921]\n",
      "25400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.087483, mse: 0.071436]\n",
      "25600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.092722, mse: 0.076680]\n",
      "25800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.086162, mse: 0.070114]\n",
      "26000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.085030, mse: 0.068981]\n",
      "26200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.082389, mse: 0.066337]\n",
      "26400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.078315, mse: 0.062259]\n",
      "26600 [D loss: 0.000001, acc: 100.00%] [G loss: 0.083701, mse: 0.067650]\n",
      "26800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.084275, mse: 0.068225]\n",
      "27000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.081697, mse: 0.065644]\n",
      "27200 [D loss: 0.000001, acc: 100.00%] [G loss: 0.085536, mse: 0.069488]\n",
      "27400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.084071, mse: 0.068021]\n",
      "27600 [D loss: 0.000010, acc: 100.00%] [G loss: 0.081698, mse: 0.065645]\n",
      "27800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.084370, mse: 0.068320]\n",
      "28000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.083483, mse: 0.067432]\n",
      "28200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.084727, mse: 0.068678]\n",
      "28400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.088350, mse: 0.072304]\n",
      "28600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.077534, mse: 0.061477]\n",
      "28800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.080959, mse: 0.064944]\n",
      "29000 [D loss: 0.000000, acc: 100.00%] [G loss: 0.085273, mse: 0.069224]\n",
      "29200 [D loss: 0.000000, acc: 100.00%] [G loss: 0.077876, mse: 0.061820]\n",
      "29400 [D loss: 0.000000, acc: 100.00%] [G loss: 0.083478, mse: 0.067427]\n",
      "29600 [D loss: 0.000000, acc: 100.00%] [G loss: 0.085512, mse: 0.069463]\n",
      "29800 [D loss: 0.000000, acc: 100.00%] [G loss: 0.085318, mse: 0.069269]\n"
     ]
    }
   ],
   "source": [
    "context_encoder = ContextEncoder()\n",
    "context_encoder.train(epochs=30000, batch_size=64, sample_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_encoder.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
