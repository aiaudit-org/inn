{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from unet_prob import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 64) 640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512, 512, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 512, 64) 36928       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512, 512, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 512, 512, 64) 36928       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512, 512, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 256, 256, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 256, 128 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 256, 256, 128 147584      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256, 256, 128 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 128 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 256 1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 256 590080      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 256 1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 512)  2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 512)  2359808     batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 512)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 1024) 4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 1024) 4096        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 1024) 9438208     batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 1024) 4096        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 1024) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 1024) 0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 512)  4719104     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 1024) 0           dropout_1[0][0]                  \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 512)  2359808     batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 512 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 256 1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 512 0           batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 256 1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 256 590080      batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 256 1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 256 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 128 295040      up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 256, 256, 128 512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 256 0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 128 295040      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 256, 256, 128 512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 128 147584      batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 256, 256, 128 512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 512, 512, 128 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 512, 512, 64) 73792       up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 512, 512, 64) 256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512, 512, 128 0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 512, 512, 64) 256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 512, 512, 64) 36928       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 512, 512, 64) 256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 512, 512, 1)  65          batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 512, 512, 1)  2           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 512, 512, 1)  0           conv2d_25[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 512, 512, 1)  2           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 512, 512, 1)  2           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512, 512, 2)  0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 33,921,415\n",
      "Trainable params: 33,907,591\n",
      "Non-trainable params: 13,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet_prob = Unet_Prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n",
      "Done with #  500\n",
      "Done with #  1000\n",
      "Done with #  1500\n",
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n",
      "Done with #  500\n",
      "Done with #  1000\n",
      "Done with #  1500\n",
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n",
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "TRAIN_PATH_IN = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/train/in/'\n",
    "VAL_PATH_IN = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/val/in/'\n",
    "TEST_PATH_IN = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/test/in/'\n",
    "\n",
    "TRAIN_PATH_REF = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/train/ref/'\n",
    "VAL_PATH_REF = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/val/ref/'\n",
    "TEST_PATH_REF = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/test/ref/'\n",
    "\n",
    "\n",
    "def get_data(path):\n",
    "    TRAIN_PATH = path\n",
    "    train_names = next(os.walk(TRAIN_PATH))[2]\n",
    "    X_train = np.zeros((len(train_names), 512, 512), dtype=np.float64)\n",
    "    print('Getting and resizing train images and masks ... ')\n",
    "    for image_name, count in zip(train_names, range(len(train_names))):\n",
    "        img = imread(TRAIN_PATH + image_name)\n",
    "        #img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "        X_train[count] = img\n",
    "        if count % 500 == 0:\n",
    "            print('Done with # ', count)\n",
    "    X_train = X_train[:,:,:,np.newaxis]\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "X_train = get_data(TRAIN_PATH_IN)\n",
    "Y_train = get_data(TRAIN_PATH_REF)\n",
    "\n",
    "X_val = get_data(VAL_PATH_IN)\n",
    "Y_val = get_data(VAL_PATH_REF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 1) (512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "#get mean\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "mean_Y_train = np.mean(Y_train, axis=0)\n",
    "\n",
    "print(mean_X_train.shape, mean_Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demean train data\n",
    "X_train -= mean_X_train\n",
    "Y_train -= mean_Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get std of demeaned train data\n",
    "std_X_train = np.std(X_train, axis = 0)\n",
    "std_Y_train = np.std(X_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize train with std\n",
    "X_train /= std_X_train\n",
    "Y_train /= std_Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demean val, normalize val with std\n",
    "X_val = (X_val-mean_X_train)/std_X_train\n",
    "Y_val = (Y_val-mean_Y_train)/std_Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale 0 to 1\n",
    "#X_train = X_train / 65535.\n",
    "#Y_train = Y_train / 65535.\n",
    "#X_val = X_val / 65535.\n",
    "#Y_val = Y_val / 65535."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.288491677418639 -8.798806808433204 18.511253623828267 -1.9262212614103575\n",
      "9.983617520588625 -8.6361820003875 17.923585546678012 -1.8948804959209244\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(X_train), np.amin(X_train), np.amax(Y_train), np.amin(Y_train))\n",
    "print(np.amax(X_val), np.amin(X_val), np.amax(Y_val), np.amin(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 70\n",
      "Train loss: -1050567.967237854\n",
      "Val loss: -1674187.254375\n",
      "Took 816.2297389507294 seconds\n",
      "\n",
      "\n",
      "Epoch 1 of 70\n",
      "Train loss: -1681532.69515625\n",
      "Val loss: -1698442.628125\n",
      "Took 810.9432067871094 seconds\n",
      "\n",
      "\n",
      "Epoch 2 of 70\n",
      "Train loss: -1704280.966015625\n",
      "Val loss: -1717878.050625\n",
      "Took 811.0682971477509 seconds\n",
      "\n",
      "\n",
      "Epoch 3 of 70\n",
      "Train loss: -1724486.909296875\n",
      "Val loss: -1737512.29875\n",
      "Took 811.0441493988037 seconds\n",
      "\n",
      "\n",
      "Epoch 4 of 70\n",
      "Train loss: -1743563.178125\n",
      "Val loss: -1754964.8775\n",
      "Took 810.9814033508301 seconds\n",
      "\n",
      "\n",
      "Epoch 5 of 70\n",
      "Train loss: -1760966.907578125\n",
      "Val loss: -1770130.315625\n",
      "Took 811.4355864524841 seconds\n",
      "\n",
      "\n",
      "Epoch 6 of 70\n",
      "Train loss: -1775568.0603125\n",
      "Val loss: -1781865.809375\n",
      "Took 811.679187297821 seconds\n",
      "\n",
      "\n",
      "Epoch 7 of 70\n",
      "Train loss: -1785245.034921875\n",
      "Val loss: -1787222.006875\n",
      "Took 811.4973247051239 seconds\n",
      "\n",
      "\n",
      "Epoch 8 of 70\n",
      "Train loss: -1789209.435625\n",
      "Val loss: -1789281.06\n",
      "Took 810.5409927368164 seconds\n",
      "\n",
      "\n",
      "Epoch 9 of 70\n",
      "Train loss: -1790626.439453125\n",
      "Val loss: -1790122.614375\n",
      "Took 807.535233259201 seconds\n",
      "\n",
      "\n",
      "Epoch 10 of 70\n",
      "Train loss: -1791688.806328125\n",
      "Val loss: -1790875.486875\n",
      "Took 802.9582929611206 seconds\n",
      "\n",
      "\n",
      "Epoch 11 of 70\n",
      "Train loss: -1792598.03265625\n",
      "Val loss: -1791392.011875\n",
      "Took 800.2212617397308 seconds\n",
      "\n",
      "\n",
      "Epoch 12 of 70\n",
      "Train loss: -1793357.533046875\n",
      "Val loss: -1792415.338125\n",
      "Took 799.6006693840027 seconds\n",
      "\n",
      "\n",
      "Epoch 13 of 70\n",
      "Train loss: -1794020.82578125\n",
      "Val loss: -1792369.106875\n",
      "Took 799.4964435100555 seconds\n",
      "\n",
      "\n",
      "Epoch 14 of 70\n",
      "Train loss: -1794614.86359375\n",
      "Val loss: -1793259.869375\n",
      "Took 799.2309610843658 seconds\n",
      "\n",
      "\n",
      "Epoch 15 of 70\n",
      "Train loss: -1795121.207890625\n",
      "Val loss: -1794494.60375\n",
      "Took 799.0649666786194 seconds\n",
      "\n",
      "\n",
      "Epoch 16 of 70\n",
      "Train loss: -1795601.90234375\n",
      "Val loss: -1794780.115\n",
      "Took 799.2414145469666 seconds\n",
      "\n",
      "\n",
      "Epoch 17 of 70\n",
      "Train loss: -1796037.372109375\n",
      "Val loss: -1795202.4725\n",
      "Took 799.6742126941681 seconds\n",
      "\n",
      "\n",
      "Epoch 18 of 70\n",
      "Train loss: -1796457.262421875\n",
      "Val loss: -1794288.81625\n",
      "Took 805.1294369697571 seconds\n",
      "\n",
      "\n",
      "Epoch 19 of 70\n",
      "Train loss: -1796862.4128125\n",
      "Val loss: -1794859.95625\n",
      "Took 798.8302602767944 seconds\n",
      "\n",
      "\n",
      "Epoch 20 of 70\n",
      "Train loss: -1797188.043984375\n",
      "Val loss: -1795041.099375\n",
      "Took 798.824253320694 seconds\n",
      "\n",
      "\n",
      "Epoch 21 of 70\n",
      "Train loss: -1797568.603984375\n",
      "Val loss: -1794429.23625\n",
      "Took 798.3811390399933 seconds\n",
      "\n",
      "\n",
      "Epoch 22 of 70\n",
      "Train loss: -1797881.809921875\n",
      "Val loss: -1796574.1675\n",
      "Took 798.4083654880524 seconds\n",
      "\n",
      "\n",
      "Epoch 23 of 70\n",
      "Train loss: -1798190.684765625\n",
      "Val loss: -1796216.6125\n",
      "Took 797.9805295467377 seconds\n",
      "\n",
      "\n",
      "Epoch 24 of 70\n",
      "Train loss: -1798485.113203125\n",
      "Val loss: -1795332.54125\n",
      "Took 796.7281603813171 seconds\n",
      "\n",
      "\n",
      "Epoch 25 of 70\n",
      "Train loss: -1798770.675546875\n",
      "Val loss: -1795347.49125\n",
      "Took 796.9967150688171 seconds\n",
      "\n",
      "\n",
      "Epoch 26 of 70\n",
      "Train loss: -1799040.072734375\n",
      "Val loss: -1797320.755625\n",
      "Took 796.5183005332947 seconds\n",
      "\n",
      "\n",
      "Epoch 27 of 70\n",
      "Train loss: -1799303.894765625\n",
      "Val loss: -1797035.33875\n",
      "Took 796.4813549518585 seconds\n",
      "\n",
      "\n",
      "Epoch 28 of 70\n",
      "Train loss: -1799536.311171875\n",
      "Val loss: -1796161.000625\n",
      "Took 796.680953502655 seconds\n",
      "\n",
      "\n",
      "Epoch 29 of 70\n",
      "Train loss: -1799785.9503125\n",
      "Val loss: -1797172.48125\n",
      "Took 796.4488923549652 seconds\n",
      "\n",
      "\n",
      "Epoch 30 of 70\n",
      "Train loss: -1799982.931015625\n",
      "Val loss: -1798035.701875\n",
      "Took 796.4358615875244 seconds\n",
      "\n",
      "\n",
      "Epoch 31 of 70\n",
      "Train loss: -1800200.4215625\n",
      "Val loss: -1797457.49\n",
      "Took 796.5964002609253 seconds\n",
      "\n",
      "\n",
      "Epoch 32 of 70\n",
      "Train loss: -1800429.431875\n",
      "Val loss: -1796498.803125\n",
      "Took 796.3705749511719 seconds\n",
      "\n",
      "\n",
      "Epoch 33 of 70\n",
      "Train loss: -1800616.53234375\n",
      "Val loss: -1798468.438125\n",
      "Took 796.2578458786011 seconds\n",
      "\n",
      "\n",
      "Epoch 34 of 70\n",
      "Train loss: -1800773.040234375\n",
      "Val loss: -1796750.235625\n",
      "Took 796.6008841991425 seconds\n",
      "\n",
      "\n",
      "Epoch 35 of 70\n",
      "Train loss: -1800919.833203125\n",
      "Val loss: -1798344.06\n",
      "Took 796.6763346195221 seconds\n",
      "\n",
      "\n",
      "Epoch 36 of 70\n",
      "Train loss: -1801143.393359375\n",
      "Val loss: -1798554.225\n",
      "Took 796.524028301239 seconds\n",
      "\n",
      "\n",
      "Epoch 37 of 70\n",
      "Train loss: -1801306.281875\n",
      "Val loss: -1797316.586875\n",
      "Took 796.6382758617401 seconds\n",
      "\n",
      "\n",
      "Epoch 38 of 70\n",
      "Train loss: -1801435.7125\n",
      "Val loss: -1797723.809375\n",
      "Took 796.5560858249664 seconds\n",
      "\n",
      "\n",
      "Epoch 39 of 70\n",
      "Train loss: -1801587.371328125\n",
      "Val loss: -1797837.77125\n",
      "Took 796.555098772049 seconds\n",
      "\n",
      "\n",
      "Epoch 40 of 70\n",
      "Train loss: -1801714.9959375\n",
      "Val loss: -1799210.52875\n",
      "Took 796.4708523750305 seconds\n",
      "\n",
      "\n",
      "Epoch 41 of 70\n",
      "Train loss: -1801860.099140625\n",
      "Val loss: -1798847.909375\n",
      "Took 796.495317697525 seconds\n",
      "\n",
      "\n",
      "Epoch 42 of 70\n",
      "Train loss: -1801995.68765625\n",
      "Val loss: -1798297.17375\n",
      "Took 796.4934692382812 seconds\n",
      "\n",
      "\n",
      "Epoch 43 of 70\n",
      "Train loss: -1802084.8746875\n",
      "Val loss: -1798290.89\n",
      "Took 796.3602476119995 seconds\n",
      "\n",
      "\n",
      "Epoch 44 of 70\n",
      "Train loss: -1802241.223984375\n",
      "Val loss: -1799193.3575\n",
      "Took 796.2433967590332 seconds\n",
      "\n",
      "\n",
      "Epoch 45 of 70\n",
      "Train loss: -1802367.711328125\n",
      "Val loss: -1799514.625\n",
      "Took 796.3402915000916 seconds\n",
      "\n",
      "\n",
      "Epoch 46 of 70\n",
      "Train loss: -1802451.98640625\n",
      "Val loss: -1797791.535\n",
      "Took 796.3916530609131 seconds\n",
      "\n",
      "\n",
      "Epoch 47 of 70\n",
      "Train loss: -1802538.9621875\n",
      "Val loss: -1798389.9325\n",
      "Took 796.459507226944 seconds\n",
      "\n",
      "\n",
      "Epoch 48 of 70\n",
      "Train loss: -1802653.607421875\n",
      "Val loss: -1798021.62625\n",
      "Took 796.4659266471863 seconds\n",
      "\n",
      "\n",
      "Epoch 49 of 70\n",
      "Train loss: -1802724.2296875\n",
      "Val loss: -1798611.155\n",
      "Took 796.3412895202637 seconds\n",
      "\n",
      "\n",
      "Epoch 50 of 70\n",
      "Train loss: -1802866.63265625\n",
      "Val loss: -1798449.01125\n",
      "Took 796.4074833393097 seconds\n",
      "\n",
      "\n",
      "Epoch 51 of 70\n",
      "Train loss: -1802909.86625\n",
      "Val loss: -1798905.25\n",
      "Took 796.4827694892883 seconds\n",
      "\n",
      "\n",
      "Epoch 52 of 70\n",
      "Train loss: -1802990.16421875\n",
      "Val loss: -1797898.783125\n",
      "Took 796.4379205703735 seconds\n",
      "\n",
      "\n",
      "Epoch 53 of 70\n",
      "Train loss: -1803127.59296875\n",
      "Val loss: -1795702.47125\n",
      "Took 796.5097851753235 seconds\n",
      "\n",
      "\n",
      "Epoch 54 of 70\n",
      "Train loss: -1803155.068984375\n",
      "Val loss: -1796523.84125\n",
      "Took 796.3952789306641 seconds\n",
      "\n",
      "\n",
      "Epoch 55 of 70\n",
      "Train loss: -1803231.253984375\n",
      "Val loss: -1796872.50625\n",
      "Took 796.4365065097809 seconds\n",
      "\n",
      "\n",
      "Epoch 56 of 70\n",
      "Train loss: -1803305.22546875\n",
      "Val loss: -1797142.469375\n",
      "Took 796.4467895030975 seconds\n",
      "\n",
      "\n",
      "Epoch 57 of 70\n",
      "Train loss: -1803386.12390625\n",
      "Val loss: -1798565.824375\n",
      "Took 796.4685566425323 seconds\n",
      "\n",
      "\n",
      "Epoch 58 of 70\n",
      "Train loss: -1803445.984609375\n",
      "Val loss: -1799862.556875\n",
      "Took 796.3819580078125 seconds\n",
      "\n",
      "\n",
      "Epoch 59 of 70\n",
      "Train loss: -1803522.02796875\n",
      "Val loss: -1799383.41625\n",
      "Took 796.361177444458 seconds\n",
      "\n",
      "\n",
      "Epoch 60 of 70\n",
      "Train loss: -1803636.3584375\n",
      "Val loss: -1800285.174375\n",
      "Took 796.1028723716736 seconds\n",
      "\n",
      "\n",
      "Epoch 61 of 70\n",
      "Train loss: -1803676.944453125\n",
      "Val loss: -1799674.946875\n",
      "Took 796.1735460758209 seconds\n",
      "\n",
      "\n",
      "Epoch 62 of 70\n",
      "Train loss: -1803758.496796875\n",
      "Val loss: -1799435.2575\n",
      "Took 796.2806334495544 seconds\n",
      "\n",
      "\n",
      "Epoch 63 of 70\n",
      "Train loss: -1803815.059296875\n",
      "Val loss: -1798747.249375\n",
      "Took 796.1341934204102 seconds\n",
      "\n",
      "\n",
      "Epoch 64 of 70\n",
      "Train loss: -1803865.063359375\n",
      "Val loss: -1800737.973125\n",
      "Took 795.974041223526 seconds\n",
      "\n",
      "\n",
      "Epoch 65 of 70\n",
      "Train loss: -1803895.138984375\n",
      "Val loss: -1799915.8025\n",
      "Took 796.1172804832458 seconds\n",
      "\n",
      "\n",
      "Epoch 66 of 70\n",
      "Train loss: -1803977.281953125\n",
      "Val loss: -1801361.604375\n",
      "Took 796.3507421016693 seconds\n",
      "\n",
      "\n",
      "Epoch 67 of 70\n",
      "Train loss: -1804025.369765625\n",
      "Val loss: -1799808.843125\n",
      "Took 796.2046020030975 seconds\n",
      "\n",
      "\n",
      "Epoch 68 of 70\n",
      "Train loss: -1804083.285859375\n",
      "Val loss: -1798803.860625\n",
      "Took 796.242945432663 seconds\n",
      "\n",
      "\n",
      "Epoch 69 of 70\n",
      "Train loss: -1804143.270546875\n",
      "Val loss: -1799786.369375\n",
      "Took 796.243171453476 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unet_prob.train(70, 1, X_train, Y_train, X_val, Y_val, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet_prob.save('unet_prob_linearprevar-linearmean-propperdatapreprocessing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet_prob.unet.load_weights('unet_prob_linearmean_linearvar.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = unet_prob.unet.get_weights(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
