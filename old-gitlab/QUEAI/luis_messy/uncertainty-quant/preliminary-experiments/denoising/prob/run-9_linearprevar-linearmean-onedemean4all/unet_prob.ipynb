{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from unet_prob import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 64) 640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512, 512, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 512, 64) 36928       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512, 512, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 512, 512, 64) 36928       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512, 512, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 256, 256, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 256, 128 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 256, 256, 128 147584      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256, 256, 128 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 128 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 256 1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 256 590080      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 256 1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 512)  2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 512)  2359808     batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 512)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 1024) 4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 1024) 4096        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 1024) 9438208     batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 1024) 4096        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 1024) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 1024) 0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 512)  4719104     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 1024) 0           dropout_1[0][0]                  \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 512)  2359808     batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 512 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 256 1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 512 0           batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 256 1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 256 590080      batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 256 1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 256 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 128 295040      up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 256, 256, 128 512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 256 0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 128 295040      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 256, 256, 128 512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 128 147584      batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 256, 256, 128 512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 512, 512, 128 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 512, 512, 64) 73792       up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 512, 512, 64) 256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512, 512, 128 0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 512, 512, 64) 256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 512, 512, 64) 36928       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 512, 512, 64) 256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 512, 512, 1)  65          batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 512, 512, 1)  2           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 512, 512, 1)  0           conv2d_25[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 512, 512, 1)  2           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 512, 512, 1)  2           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512, 512, 2)  0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 33,921,415\n",
      "Trainable params: 33,907,591\n",
      "Non-trainable params: 13,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet_prob = Unet_Prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n",
      "Done with #  500\n",
      "Done with #  1000\n",
      "Done with #  1500\n",
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n",
      "Done with #  500\n",
      "Done with #  1000\n",
      "Done with #  1500\n",
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n",
      "Getting and resizing train images and masks ... \n",
      "Done with #  0\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "TRAIN_PATH_IN = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/train/in/'\n",
    "VAL_PATH_IN = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/val/in/'\n",
    "TEST_PATH_IN = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/test/in/'\n",
    "\n",
    "TRAIN_PATH_REF = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/train/ref/'\n",
    "VAL_PATH_REF = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/val/ref/'\n",
    "TEST_PATH_REF = '/media/oala/4TB/DATA/experiments-hhi/uncertainty-quant/ellipsoid-toy/denoising/test/ref/'\n",
    "\n",
    "\n",
    "def get_data(path):\n",
    "    TRAIN_PATH = path\n",
    "    train_names = next(os.walk(TRAIN_PATH))[2]\n",
    "    X_train = np.zeros((len(train_names), 512, 512), dtype=np.float64)\n",
    "    print('Getting and resizing train images and masks ... ')\n",
    "    for image_name, count in zip(train_names, range(len(train_names))):\n",
    "        img = imread(TRAIN_PATH + image_name)\n",
    "        #img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "        X_train[count] = img\n",
    "        if count % 500 == 0:\n",
    "            print('Done with # ', count)\n",
    "    X_train = X_train[:,:,:,np.newaxis]\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "X_train = get_data(TRAIN_PATH_IN)\n",
    "Y_train = get_data(TRAIN_PATH_REF)\n",
    "\n",
    "X_val = get_data(VAL_PATH_IN)\n",
    "Y_val = get_data(VAL_PATH_REF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale 0 to 1\n",
    "X_train = X_train / 65535.\n",
    "Y_train = Y_train / 65535.\n",
    "X_val = X_val / 65535.\n",
    "Y_val = Y_val / 65535."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0 1.0 0.0\n",
      "1.0 0.0 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(X_train), np.amin(X_train), np.amax(Y_train), np.amin(Y_train))\n",
    "print(np.amax(X_val), np.amin(X_val), np.amax(Y_val), np.amin(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 1) (512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "#get mean\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "mean_Y_train = np.mean(Y_train, axis=0)\n",
    "\n",
    "print(mean_X_train.shape, mean_Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demean train data\n",
    "X_train -= mean_X_train\n",
    "Y_train -= mean_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get std of demeaned train data\n",
    "#std_X_train = np.std(X_train, axis = 0)\n",
    "#std_Y_train = np.std(X_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize train with std\n",
    "#X_train /= std_X_train\n",
    "#Y_train /= std_Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demean val, normalize val with std\n",
    "#X_val = (X_val-mean_X_train)/std_X_train\n",
    "#Y_val = (Y_val-mean_Y_train)/std_Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demean val\n",
    "X_val = X_val-mean_X_train\n",
    "Y_val = Y_val-mean_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5868481059739072 -0.5111684023804072 0.5846666857404434 -0.5110668631265736\n",
      "0.5865759613183797 -0.5115774204623481 0.5831474307621882 -0.5109926565957121\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(X_train), np.amin(X_train), np.amax(Y_train), np.amin(Y_train))\n",
    "print(np.amax(X_val), np.amin(X_val), np.amax(Y_val), np.amin(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 70\n",
      "Train loss: -1085166.8852467346\n",
      "Val loss: -1685637.851875\n",
      "Took 815.1719236373901 seconds\n",
      "\n",
      "\n",
      "Epoch 1 of 70\n",
      "Train loss: -1700406.155703125\n",
      "Val loss: -1711778.244375\n",
      "Took 811.6183607578278 seconds\n",
      "\n",
      "\n",
      "Epoch 2 of 70\n",
      "Train loss: -1722159.0146875\n",
      "Val loss: -1732491.71125\n",
      "Took 811.2653868198395 seconds\n",
      "\n",
      "\n",
      "Epoch 3 of 70\n",
      "Train loss: -1742758.176484375\n",
      "Val loss: -1752903.38375\n",
      "Took 811.6378870010376 seconds\n",
      "\n",
      "\n",
      "Epoch 4 of 70\n",
      "Train loss: -1762741.729140625\n",
      "Val loss: -1772338.48125\n",
      "Took 811.2536182403564 seconds\n",
      "\n",
      "\n",
      "Epoch 5 of 70\n",
      "Train loss: -1781506.815546875\n",
      "Val loss: -1790372.9075\n",
      "Took 810.9599113464355 seconds\n",
      "\n",
      "\n",
      "Epoch 6 of 70\n",
      "Train loss: -1798668.465\n",
      "Val loss: -1806074.35625\n",
      "Took 811.7238945960999 seconds\n",
      "\n",
      "\n",
      "Epoch 7 of 70\n",
      "Train loss: -1808694.762890625\n",
      "Val loss: -1809465.6525\n",
      "Took 811.2783348560333 seconds\n",
      "\n",
      "\n",
      "Epoch 8 of 70\n",
      "Train loss: -1809937.6815625\n",
      "Val loss: -1809842.451875\n",
      "Took 811.3549008369446 seconds\n",
      "\n",
      "\n",
      "Epoch 9 of 70\n",
      "Train loss: -1810187.94734375\n",
      "Val loss: -1809958.075625\n",
      "Took 811.5753433704376 seconds\n",
      "\n",
      "\n",
      "Epoch 10 of 70\n",
      "Train loss: -1810289.81921875\n",
      "Val loss: -1809864.865625\n",
      "Took 811.6844711303711 seconds\n",
      "\n",
      "\n",
      "Epoch 11 of 70\n",
      "Train loss: -1810349.911015625\n",
      "Val loss: -1809928.088125\n",
      "Took 811.7163751125336 seconds\n",
      "\n",
      "\n",
      "Epoch 12 of 70\n",
      "Train loss: -1810388.6228125\n",
      "Val loss: -1810091.9175\n",
      "Took 811.6486842632294 seconds\n",
      "\n",
      "\n",
      "Epoch 13 of 70\n",
      "Train loss: -1810414.508203125\n",
      "Val loss: -1809562.415\n",
      "Took 811.7260799407959 seconds\n",
      "\n",
      "\n",
      "Epoch 14 of 70\n",
      "Train loss: -1810433.897265625\n",
      "Val loss: -1809631.408125\n",
      "Took 811.895749092102 seconds\n",
      "\n",
      "\n",
      "Epoch 15 of 70\n",
      "Train loss: -1810448.79015625\n",
      "Val loss: -1810132.39625\n",
      "Took 811.7884845733643 seconds\n",
      "\n",
      "\n",
      "Epoch 16 of 70\n",
      "Train loss: -1810460.707890625\n",
      "Val loss: -1809935.3575\n",
      "Took 811.7772688865662 seconds\n",
      "\n",
      "\n",
      "Epoch 17 of 70\n",
      "Train loss: -1810470.725234375\n",
      "Val loss: -1809382.591875\n",
      "Took 811.8980047702789 seconds\n",
      "\n",
      "\n",
      "Epoch 18 of 70\n",
      "Train loss: -1810476.849921875\n",
      "Val loss: -1808866.76875\n",
      "Took 811.923534154892 seconds\n",
      "\n",
      "\n",
      "Epoch 19 of 70\n",
      "Train loss: -1810485.79234375\n",
      "Val loss: -1808948.624375\n",
      "Took 811.4570107460022 seconds\n",
      "\n",
      "\n",
      "Epoch 20 of 70\n",
      "Train loss: -1810490.397734375\n",
      "Val loss: -1809793.89\n",
      "Took 811.2225840091705 seconds\n",
      "\n",
      "\n",
      "Epoch 21 of 70\n",
      "Train loss: -1810494.9728125\n",
      "Val loss: -1809891.175\n",
      "Took 810.8506698608398 seconds\n",
      "\n",
      "\n",
      "Epoch 22 of 70\n",
      "Train loss: -1810499.809296875\n",
      "Val loss: -1807826.208125\n",
      "Took 810.3311879634857 seconds\n",
      "\n",
      "\n",
      "Epoch 23 of 70\n",
      "Train loss: -1810503.974453125\n",
      "Val loss: -1808064.611875\n",
      "Took 809.5909395217896 seconds\n",
      "\n",
      "\n",
      "Epoch 24 of 70\n",
      "Train loss: -1810506.33171875\n",
      "Val loss: -1809197.29625\n",
      "Took 808.5721924304962 seconds\n",
      "\n",
      "\n",
      "Epoch 25 of 70\n",
      "Train loss: -1810511.77609375\n",
      "Val loss: -1809877.380625\n",
      "Took 807.8076701164246 seconds\n",
      "\n",
      "\n",
      "Epoch 26 of 70\n",
      "Train loss: -1810515.48421875\n",
      "Val loss: -1808722.835\n",
      "Took 806.8658277988434 seconds\n",
      "\n",
      "\n",
      "Epoch 27 of 70\n",
      "Train loss: -1810518.933828125\n",
      "Val loss: -1809245.22125\n",
      "Took 805.7985687255859 seconds\n",
      "\n",
      "\n",
      "Epoch 28 of 70\n",
      "Train loss: -1810519.96859375\n",
      "Val loss: -1808795.338125\n",
      "Took 805.3795101642609 seconds\n",
      "\n",
      "\n",
      "Epoch 29 of 70\n",
      "Train loss: -1810524.87046875\n",
      "Val loss: -1809426.60875\n",
      "Took 804.5545239448547 seconds\n",
      "\n",
      "\n",
      "Epoch 30 of 70\n",
      "Train loss: -1810525.47984375\n",
      "Val loss: -1809456.800625\n",
      "Took 803.9279971122742 seconds\n",
      "\n",
      "\n",
      "Epoch 31 of 70\n",
      "Train loss: -1810526.603828125\n",
      "Val loss: -1808302.54875\n",
      "Took 803.4945294857025 seconds\n",
      "\n",
      "\n",
      "Epoch 32 of 70\n",
      "Train loss: -1810532.00484375\n",
      "Val loss: -1808377.16375\n",
      "Took 803.0852084159851 seconds\n",
      "\n",
      "\n",
      "Epoch 33 of 70\n",
      "Train loss: -1810532.35671875\n",
      "Val loss: -1809436.160625\n",
      "Took 802.4834334850311 seconds\n",
      "\n",
      "\n",
      "Epoch 34 of 70\n",
      "Train loss: -1810535.834375\n",
      "Val loss: -1809286.611875\n",
      "Took 802.3718147277832 seconds\n",
      "\n",
      "\n",
      "Epoch 35 of 70\n",
      "Train loss: -1810536.246015625\n",
      "Val loss: -1809608.693125\n",
      "Took 802.3189890384674 seconds\n",
      "\n",
      "\n",
      "Epoch 36 of 70\n",
      "Train loss: -1810539.868828125\n",
      "Val loss: -1806557.59625\n",
      "Took 802.0968098640442 seconds\n",
      "\n",
      "\n",
      "Epoch 37 of 70\n",
      "Train loss: -1810539.8259375\n",
      "Val loss: -1808828.0725\n",
      "Took 802.0752909183502 seconds\n",
      "\n",
      "\n",
      "Epoch 38 of 70\n",
      "Train loss: -1810541.940703125\n",
      "Val loss: inf\n",
      "Took 801.9302358627319 seconds\n",
      "\n",
      "\n",
      "Epoch 39 of 70\n",
      "Train loss: -1810543.439765625\n",
      "Val loss: -1809797.185625\n",
      "Took 801.8804929256439 seconds\n",
      "\n",
      "\n",
      "Epoch 40 of 70\n",
      "Train loss: -1810547.654609375\n",
      "Val loss: -1808343.86875\n",
      "Took 801.6953682899475 seconds\n",
      "\n",
      "\n",
      "Epoch 41 of 70\n",
      "Train loss: -1810547.216328125\n",
      "Val loss: -1808912.79625\n",
      "Took 802.0740926265717 seconds\n",
      "\n",
      "\n",
      "Epoch 42 of 70\n",
      "Train loss: -1810547.923515625\n",
      "Val loss: -1809371.826875\n",
      "Took 801.2691543102264 seconds\n",
      "\n",
      "\n",
      "Epoch 43 of 70\n",
      "Train loss: -1810551.304609375\n",
      "Val loss: inf\n",
      "Took 801.3525638580322 seconds\n",
      "\n",
      "\n",
      "Epoch 44 of 70\n",
      "Train loss: -1810551.224453125\n",
      "Val loss: -1809792.910625\n",
      "Took 801.0707867145538 seconds\n",
      "\n",
      "\n",
      "Epoch 45 of 70\n",
      "Train loss: -1810553.2384375\n",
      "Val loss: -1808958.65625\n",
      "Took 800.985805273056 seconds\n",
      "\n",
      "\n",
      "Epoch 46 of 70\n",
      "Train loss: -1810552.98515625\n",
      "Val loss: inf\n",
      "Took 801.1781148910522 seconds\n",
      "\n",
      "\n",
      "Epoch 47 of 70\n",
      "Train loss: -1810555.65578125\n",
      "Val loss: inf\n",
      "Took 801.1683657169342 seconds\n",
      "\n",
      "\n",
      "Epoch 48 of 70\n",
      "Train loss: -1810555.8796875\n",
      "Val loss: -1809766.39875\n",
      "Took 801.1065530776978 seconds\n",
      "\n",
      "\n",
      "Epoch 49 of 70\n",
      "Train loss: -1810558.6428125\n",
      "Val loss: -1802671.33375\n",
      "Took 801.2223408222198 seconds\n",
      "\n",
      "\n",
      "Epoch 50 of 70\n",
      "Train loss: -1810559.3396875\n",
      "Val loss: -1801934.570625\n",
      "Took 801.0211219787598 seconds\n",
      "\n",
      "\n",
      "Epoch 51 of 70\n",
      "Train loss: -1810559.316328125\n",
      "Val loss: inf\n",
      "Took 800.7869238853455 seconds\n",
      "\n",
      "\n",
      "Epoch 52 of 70\n",
      "Train loss: -1810559.286953125\n",
      "Val loss: inf\n",
      "Took 801.0093863010406 seconds\n",
      "\n",
      "\n",
      "Epoch 53 of 70\n",
      "Train loss: -1810562.908125\n",
      "Val loss: inf\n",
      "Took 800.7516784667969 seconds\n",
      "\n",
      "\n",
      "Epoch 54 of 70\n",
      "Train loss: -1810561.90625\n",
      "Val loss: inf\n",
      "Took 800.8066284656525 seconds\n",
      "\n",
      "\n",
      "Epoch 55 of 70\n",
      "Train loss: -1810562.957890625\n",
      "Val loss: -1800650.570625\n",
      "Took 800.7184209823608 seconds\n",
      "\n",
      "\n",
      "Epoch 56 of 70\n",
      "Train loss: -1810563.17234375\n",
      "Val loss: inf\n",
      "Took 800.7340397834778 seconds\n",
      "\n",
      "\n",
      "Epoch 57 of 70\n",
      "Train loss: -1810564.636640625\n",
      "Val loss: inf\n",
      "Took 800.8041734695435 seconds\n",
      "\n",
      "\n",
      "Epoch 58 of 70\n",
      "Train loss: -1810564.9115625\n",
      "Val loss: inf\n",
      "Took 800.6912467479706 seconds\n",
      "\n",
      "\n",
      "Epoch 59 of 70\n",
      "Train loss: -1810566.372421875\n",
      "Val loss: -1809772.149375\n",
      "Took 800.5730862617493 seconds\n",
      "\n",
      "\n",
      "Epoch 60 of 70\n",
      "Train loss: -1810566.14078125\n",
      "Val loss: inf\n",
      "Took 800.5951051712036 seconds\n",
      "\n",
      "\n",
      "Epoch 61 of 70\n",
      "Train loss: -1810568.253125\n",
      "Val loss: inf\n",
      "Took 800.5510709285736 seconds\n",
      "\n",
      "\n",
      "Epoch 62 of 70\n",
      "Train loss: -1810567.1690625\n",
      "Val loss: -1808456.431875\n",
      "Took 800.4666900634766 seconds\n",
      "\n",
      "\n",
      "Epoch 63 of 70\n",
      "Train loss: -1810568.892265625\n",
      "Val loss: -1805079.03625\n",
      "Took 800.3307039737701 seconds\n",
      "\n",
      "\n",
      "Epoch 64 of 70\n",
      "Train loss: -1810569.434140625\n",
      "Val loss: inf\n",
      "Took 800.5090925693512 seconds\n",
      "\n",
      "\n",
      "Epoch 65 of 70\n",
      "Train loss: -1810569.270703125\n",
      "Val loss: -1808439.364375\n",
      "Took 800.4370718002319 seconds\n",
      "\n",
      "\n",
      "Epoch 66 of 70\n",
      "Train loss: -1810569.22453125\n",
      "Val loss: -1807921.33\n",
      "Took 800.4326360225677 seconds\n",
      "\n",
      "\n",
      "Epoch 67 of 70\n",
      "Train loss: -1810568.800234375\n",
      "Val loss: -1809695.566875\n",
      "Took 800.3647813796997 seconds\n",
      "\n",
      "\n",
      "Epoch 68 of 70\n",
      "Train loss: -1810571.99296875\n",
      "Val loss: -1809350.7975\n",
      "Took 800.4930603504181 seconds\n",
      "\n",
      "\n",
      "Epoch 69 of 70\n",
      "Train loss: -1810572.153046875\n",
      "Val loss: inf\n",
      "Took 800.4983258247375 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unet_prob.train(70, 1, X_train, Y_train, X_val, Y_val, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_prob.save('unet_prob_linearprevar-linearmean-onedemean4all.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet_prob.unet.load_weights('unet_prob_linearmean_linearvar.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = unet_prob.unet.get_weights(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
