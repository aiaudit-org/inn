\documentclass{article}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}


\usepackage{amsmath}
\usepackage{color}
\usepackage{graphicx}

%tikz
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows}


\newcommand{\memo}[1]{{\bf \textcolor{blue}{[#1]}}}

\title{Experiments to determine the 'singular values' of a neural network}
\date{}

\begin{document}
\maketitle

\section*{Experiment 1}
The first experiment revolved around using the quasi linear behaviour of the neural network around the data points to deduce its effects on the basis vectors of the SVD of the forward operator.\\
\par
Determine the SVD $U\Sigma V^T=A$ of the forward operator.\\
Then use the data $y_i=Ax_i + e$ with noise $e$ with standard deviation $\eta$ to reconstruct the basis vectors $u_j$ from the SVD by finding $c_{ji}$ such that
\begin{align*}
u_j = \sum_i c_{ji} y_i.
\end{align*}
Then define $\tilde{u}_j$ as
\begin{align*}
\tilde{u}_j := \sum_i c_{ji} \Phi_\theta (y_i)
\end{align*}
where $\Phi_\theta$ denotes the trained neural network.\\
In the ideal case of no noise and a perfectly trained network such that $\Phi_\theta(y_i)=x_i$ we can expect $\tilde{u}_j = \frac{1}{\sigma_j} v_j$ where $\sigma_j$ is the $j$th singular value of the forward operator as then
\begin{align*}
\tilde{u}_j &= \sum_i c_{ji} \Phi_\theta(y_i) = \sum_i c_{ji} x_i\\
&= A^+ \left(\sum_i c_{ji} y_i\right) = V\Sigma^{-1} U^T u_j = v_j \frac{1}{\sigma_j}
\end{align*}
In our experiment we compute $\left\langle \tilde{u}_j, v_j \right\rangle$ to yield the 'singular values' of our network acting on the data.\\
The results showed these values to coincide with the spectral values of $A^+$ for the most part but these values could also be negative and were pretty much unpredictable for the higher singular values.
This can be attributed to the noise found on the data for which the network compensated yielding weird effects on the high frequency singular values.\\
\par
It can be argued that if the network is trained sufficiently well, this method will automatically yield the singular values of the pseudoinverse if there is no noise and will show some unpredictable decay in the high frequency singular values if there is noise on the data as is cant be represented correctly for all data points by the basis vectors of the SVD of the forward operator.

\section*{Experiment 2}
The second experiment revolved around using the singular value decomposition of the network acting on one input and extract the singular values. All this was done with the assumption that the networks singular vectors coincide with the ones of the pseudoinverse.\\
\par
For the output of the network we could write
\begin{align*}
\Phi_\theta (y_i) = \sum_j f_j(y_i) \left\langle y_i, u_j \right\rangle v_j
\end{align*}
with $f$ being the function that yields the 'correct' singular values for each input $y_i$.\\
As $(v_j)_j$ forms an ONB we could just identify
\begin{align*}
f_j(y_i) \left\langle y_i, u_j \right\rangle = \left\langle\Phi_\theta(y_i),v_j\right\rangle.
\end{align*}
Therefore we could extract the singular values for each data point by
\begin{align*}
f_j(y_i) = \frac{\left\langle\Phi_\theta(y_i),v_j\right\rangle}{\left\langle y_i, u_j \right\rangle}
\end{align*}

This method yields different singular values for each data point which are interestingly very different in behaviour from the ones computed by the first method. They showed a less prominent decay but and an even more unpredictable behaviour for the high frequency singular values (which very well could be negative too).


\section*{Experiment 3}
Analysing the local linear approximation of the neural network in each data point.\\
\par
For this a network was trained without bias.
Then for a fixed input $y_i$ either the matrix is reconstructed from the gradient at the point $y_i$ or just by the linear operator which is generated from the neural network by fixing the ReLU depending on if it 'fired' or not during the forward propagation of $y_i$.\\
\par
The results from this seemed not to be very informative.
First of all the linear operator generated by this method had pretty much nothing to do with forward operator and as such its effect on the basis vectors of the SVD of the forward operator were at least somewhat similar in their frequency range but apart from that not really similar. Additionally the SVD of this generated operator had basis vectors which all very very spiky and didn't have any distinct frequency range which was certainly the case for the forward operator.\
\par
One thing which I also tried for this experiment to take the SVD of this generated operator, and computing $U\Sigma^2 V^T$, basically 'weighting' the singular values by themselves resulting in higher values being enhanced much more than lower values.
This operator evaluated on the same input yielded a very noisy output which was distinctly higher at the jumps of the target which one could interpret as uncertain.

\end{document}