\documentclass{article}


\usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{tcolorbox}
\usepackage{amssymb}


\usepackage{amsmath}
\usepackage{color}
\usepackage{graphicx}

%tikz
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations.pathreplacing}

\setlength\parindent{0pt}
\newcommand{\memo}[1]{{\bf \textcolor{blue}{[#1]}}}

\title{The Uncertainty Formula}
\date{}
\author{}

	\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
	\DeclareMathOperator{\R}{\mathbb{R}}
	
	\DeclareMathOperator{\K}{\mathbb{K}}
	\DeclareMathOperator{\C}{\mathbb{C}}
	\DeclareMathOperator{\Z}{\mathbb{Z}}

\begin{document}
\maketitle
% 

\section*{Setup}

We will denote by
\begin{align*}
&x_0\in\R^n = :X, \qquad &&\text{the unknown image,}\\
&y_0 = Ax_0+ \eta \in \R^m =:Y, \qquad &&\text{the noisy inverse problem with a noise magnitude of $\sigma$, i.e.} \|\eta\|_2\leq\sigma,\\
&\pi (x,y), \qquad &&\text{the joint probability density on $X\times Y$,}\\
&\pi_Y (y) = \int_{\R^n} \pi (x,y) dx, \qquad &&\text{the marginalized probability density on the measurement space,}\\
&\pi_X(x) = \int_{\R^m} \pi (x,y) dy, \qquad &&\text{the marginalized probability density on the image space,}\\
&\pi_\mathcal{E}(x), \qquad &&\text{the probability density of the noise $\eta$ on the measurements,}\\
&\Phi_\theta : \R^m \to \R^n, \qquad &&\text{the (robust) inversion model with parameters $\theta$.}\\
\end{align*}

\begin{tcolorbox}[title=ToDo]
\begin{itemize}
 \item check literature (DL and Bayesian IPs) for related concepts; 
 \item derive useful statements (e.g.~explanations for different error sources) from \eqref{eq:main};
 \item try to systematically nail down (and define) different sources for uncertainty;
 \item think about practical ways for using \eqref{eq:main} (see maybe Adler\&Ã–kten);
 
 $\leadsto$ do experiments!
\end{itemize}
\end{tcolorbox}
% 

\section*{Main}
We propose to define the uncertainty $\mathcal{U}$ in $y_0$ as
\begin{align}
\mathcal{U}(y_0) = \mathbb{E}\left[\ \mathcal{L}(\Phi_\theta (Z_{y_0}),X_{y_0})^p \  \right]^{\frac{1}{p}}
\label{eq:main}
\end{align}
With $Z_{y_0}$ and $X_{y_0}$ being two independent random variables being distributed by
\begin{align*}
Z_{y_0}\ &\sim\ \pi_Y \cdot g_{y_0} \\
X_{y_0}\ &\sim\ \pi\left(\ \cdot\ | y_0\right) = \frac{\pi(\cdot,y_0)}{\pi_Y(y_0)},
\end{align*}
where $g_{y_0}$ is for instance given by
\[
 g_{y_0}( y) = \left(\pi_Y(y) \cdot  \chi_{[-\varepsilon / \sqrt{n} + y_0,\varepsilon / \sqrt{n} + y_0]} (y) \right)  / \pi_Y ([-\varepsilon / \sqrt{n} + y_0,\varepsilon / \sqrt{n} + y_0]),
 \]
 i.e., we are localizing the data distribution $\pi_Y$ around an $\varepsilon$-environment of $y_0$. 
 Note that the distribution $\pi\left(\ \cdot\ | y_0\right)$ is the posterior probability distribution of $X$ given the observed data $y_0$, i.e., precisely what Bayesian methods are aiming for.\\
 
At first sight this might seem arbitrary but it becomes more clear once one looks at special cases of this formula.
For example, let $p=1$ and the loss be the squared difference.
Then the uncertainty in $y_0$ simplifies to
\begin{align*}
\mathcal{U}(y_0) = \mathbb{E}\left[\ (\Phi_\theta (Z_{y_0})-X_{y_0})^2 \ \right].
\end{align*}
This now has the following interpretation:\\
the random variable $Z_{y_0}$ takes on values in the $\epsilon$-neighbourhood of $y_0$ with the probability induced by $\pi_Y$. This means that points in the neighbourhood of $y_0$ which are not in the image of $A$ or their pre-image has very low probability are not taken into consideration.
This can also be interpreted as restricting the neighbourhood of $y_0$ to the ``data manifold''.\\
$X_{y_0}$ on the other hand takes on all values which map to $y_0$ with their respective probability given by $\pi_X$, i.e., the posterior distribution in a Bayes setting.\\
The uncertainty is the the mean difference between what the network predicts in this neighbourhood of $y_0$ and the actual values which map to $y_0$.
\\
\par
Under the assumption that the measurement $y_0$ is generated by
$A x_0 + \eta$ we can write the posterior as follows:
\begin{align*}
\pi_X(x|y_0) &= \frac{\pi_X(x)\cdot \pi_Y(y_0|x)}{\pi_Y(y_0)}\\
&= \frac{\pi_X(x)\cdot \pi_Y(y_0|x)}{\int_{\R^m} \pi_X(z)\cdot \pi_Y(y_0|z) dz}\\
\\
&= \frac{\pi_X(x)\cdot \pi_\mathcal{E}(y_0-Ax)}{\int_{\R^m} \pi_X(z)\cdot \pi_\mathcal{E}(y_0-Az) dz},
\end{align*}
where $\pi_X(x)$ is the prior on the source data and $\pi_\mathcal{E}(y_0-Ax)$ can be viewed as the pullback of the noise distribution around $y_0$ under $A$.\\
\par
Additionally, we can decompose the simplified uncertainty from above into the parts of a Variance-Bias-Tradeoff:
\begin{align*}
&\mathbb{E}\left[\ (\Phi_\theta (Z_{y_0})-X_{y_0})^2 \ \right]\\
&= \mathbb{V}\left[\Phi_\theta(Z_{y_0}) \right] + \mathbb{V}\left[X_{y_0} \right]+\left(\mathbb{E}\left[\Phi_\theta(Z_{y_0}) \right]-\mathbb{E}\left[X_{y_0} \right]\right)^2.
\intertext{By now seeing $Z_{y_0}$ as the random variable of values in $R^m$ which produced $y_0$ after the addition of the noise $\eta$ multiplied by the prior of $Y$, we can assume $Z_{y_0}$ being distributed as $AX_{y_0}$:}
&=\mathbb{V}\left[\Phi_\theta(AX_{y_0}) \right] + \mathbb{V}\left[X_{y_0} \right]+\left(\mathbb{E}\left[\Phi_\theta(AX_{y_0}) \right]-\mathbb{E}\left[X_{y_0} \right]\right)^2.
\end{align*}


\begin{tcolorbox}[title=Plan]
\begin{itemize}
 \item group meeting planed from 30.09. to 02.10.;
 \item investigate ToDo's above;
 \item take time for reading;
 \item Goal 1): come up with a strategy;
 \item Goal 2): distribution of tasks;
\end{itemize}
\end{tcolorbox}


\end{document}
