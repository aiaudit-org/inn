\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage{geometry}

\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{cleveref}

\input{abbrv}

\title{Variational Bayesian Methods for Neural Networks}
\author{placeholder}

%%%%% %%%%% %%%%% %%%%% %%%%%
\begin{document}

\maketitle

\section{Introduction to Variational Bayesian Methods}
Consider a statistical model for random variables $(X,Y)$ following an
unknown joint probability distribution on a space $\CX\times\CY\subset\R^n\times\R^d$: Assume
that there is a function $f_W\colon\CX\to\CY$ parametrized by a random variable $W$ (e.g. a neural network
parametrized by weights and biases) such that $p(Y\vert X,W)$ follows
a ``simple'' probability distribution of our choice (e.g. Gaussian) depending on $X$ and $W$ through $f_W(X)$.

Given a sample $(\bfX, \bfY)=((\bfx_1,\dots,\bfx_N),(\bfy_1,\dots,\bfy_N))$ of independent realizations of $(X, Y)$
we have
\begin{equation}\label{eq:weights_post}
    p(W\vert\bfX,\bfY) = \frac{p(\bfY\vert\bfX,W) p(W)}{p(Y\vert\bfX)} = \frac{p(\bfY\vert\bfX,W) p(W)}{\int p(Y\vert\bfX, W) p(W)\,\D W}.
\end{equation}
Given a new, previously unseen realization $\bfx^\ast$ of $X$ we then use this
posterior in the inference step to obtain the distribution
\begin{equation}\label{eq:y_post}
    p(\bfy^\ast\vert\bfx^\ast,\bfX,\bfY) = \int p(\bfy^\ast\vert\bfx^\ast, W) p(W\vert\bfX,\bfY)\,\D W.
\end{equation}
The expression in \cref{eq:weights_post} is typically untractable as it involes
solving an integral over all possible parameter assignments. In variational
Bayesian methods, the parameter posterior is approximated by another distribution
$q(W)\approx p(W\vert\bfX,\bfY)$, which is then also used during inference.
For this we choose a family of feasible distributions
$q_\theta$ parametrized by another set of parameters $\theta$ and minimize the KL-divergence
between $q_\theta(W)$ and $p(W\vert\bfX,\bfY)$. This is equivalent to maximizing the \emph{evidence lower bound} (ELBO)
\begin{equation}\label{eq:elbo}
  \CL(\theta) = \int q_\theta(W) \log p(\bfY\vert \bfX, W)\,\D W - \KL(q_\theta(W)\vert\vert p(W)),
\end{equation}
since we have
\[\KL(q_\theta(W)\vert\vert p(W\vert\bfX,\bfY)) = - \CL(\theta) + \log p(\bfY\vert\bfX).\]
Thus, the setup of a Variational Bayes method boils down to the following steps:
\begin{enumerate}
  \item choose a family of parametrized functions $f_W\colon\CX\to\CY$,
  \item choose a distribution $p(Y\vert X,W)$ depending on $X$ and $W$ via $f_W(X)$,
  \item choose a prior $p(W)$
  \item choose a family of approximating distibutions $q_\theta$ parametrized by $\theta$
  \item (approximately) find a maximizer $q^\ast$ of the ELBO in \cref{eq:elbo},
  \item (approximately) calculate the integral in \cref{eq:y_post} (with $p(W\vert\bfX,\bfY)$ replaced by $q^\ast(W)$) for inference.
\end{enumerate}

\section{Gal's MC Dropout}
\begin{enumerate}
  \item $f_W\colon\CX\to\CY$ is a $L$-layer feedforward neural network parametrized by its weights and biases, i.e.\ $W=((\bfA_i, \bfb_i))_{i=1}^{L}$
  \item $p(Y\vert X,W)$ is a Gaussian density with mean $f_W(X)$ and a fixed precision parameter $\tau$, i.e. $Y\vert X,W \sim \CN(f_W(X), \tau^{-2}\bfI)$,
  \item $p(W)$ is Gaussian, i.e. $W \sim \CN(0, \ell^{-2}\bfI)$,
  \item $q_\theta$ is parametrized by deterministic weights and biases $\theta=((\bfM_i, \bfm_i))_{i=1}^{L}$ that are multiplied by independent Bernoulli variables, that is
  for each layer $i$, we fix a $0<p_i<1$ and have $q((\bfA_i)_{j,k}=(\bfM_i)_{j,k}) = p_i$ and $q((\bfA_i)_{j,k}=0) = 1- p_i$ and analogously for the biases,
  \item the ELBO in \cref{eq:elbo} is maximized by approximating the integral by taking a single sample from $q(W)$ per
  training step, which amounts to the Dropout objective (plus regularization terms coming from the KL divergence to the prior, i.e. weight decay),
  \item the inference step in \cref{eq:y_post} (with $p(W\vert\bfX,\bfY)$ replaced by $q^\ast(W)$) is approximated by Monte Carlo integration, i.e.\ sampling multiple times from $q^\ast(W)$ (this is called MC Dropout or Test Time Dropout).
\end{enumerate}

\section{Our Interval Approach}

\begin{enumerate}
  \item $f_W\colon\CX\to\CY$ is a $L$-layer feedforward neural network parametrized by its weights and biases, i.e.\ $W=((\bfA_i, \bfb_i))_{i=1}^{L}$
  \item $p(Y\vert X,W)$ is a Gaussian density with mean $f_W(X)$ and a fixed precision parameter $\tau$, i.e. $Y\vert X,W \sim \CN(f_W(X), \tau^{-2}\bfI)$,
  \item $p(W)$ remains to be chosen (it could also be Gaussian as in Gal's approach, which leads to weight decay regularization),
  \item $q_\theta$ is parametrized by deterministic upper and lower bounds for the weights and biases $\theta=((\underline{\bfM_i}, \overline{\bfM_i}, \underline{\bfm_i}, \overline{\bfm_i}))_{i=1}^{L}$, and $q$ is any distribution of weights and biases supported within the specified intervals,
  \item the ELBO in \cref{eq:elbo} is maximized by approximating the integral with its lower and upper bound using
  the specified intervals (see \cref{sec:interval_elbo}),
  \item the inference step in \cref{eq:y_post} (with $p(W\vert\bfX,\bfY)$ replaced by $q^\ast(W)$) is approximated by again using lower and upper bounds derived from the intervals (see \cref{sec:interval_inference})
\end{enumerate}

\subsection{Interval ELBO Approximation}\label{sec:interval_elbo}
Based on the parameters $\theta$ specifying the weight and bias itervals we can calculate the range of possible values of $f_W(\bfx)$ for any fixed input $\bfx$ and $W$ distributed according to $q_\theta(W)$. We denote this as $[\underline{f_W(\bfx)}, \overline{f_W(\bfx)}]$ (and the interval is actually a box and meant component-wise). For any fixed $\bfx\in\CX$, $\bfy\in\CY$ let us denote the best and worst approximation of $\bfy$ within this box by
\[\underline{\bfw}(\bfy,\bfx) = \argmin_{W\sim q(W)} \|f_W(\bfx) - \bfy \|_2^2\]
and
\[\overline{\bfw}(\bfy,\bfx) = \argmax_{W\sim q(W)} \|f_W(\bfx) - \bfy \|_2^2.\]
Now we can estimate the integral (i.e. the ELBO without the KL-divergence term) by
\begin{align*}
    \int q_\theta(W) \log p(\bfY\vert \bfX, W)\,\D W &= \int q_\theta(W) \log \prod_{i=1}^N \frac{1}{C} e^{-\frac{\tau^{2d}}{2}\|f_W(\bfx_i)-\bfy_i\|_2^2} \,\D W \\
    &\leq -N\log(C) - \sum_{i=1}^N \frac{\tau^{2d}}{2}\|f_{\underline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i)-\bfy_i\|_2^2,
\end{align*}
where $C=(2\pi \tau^{-2})^{d/2}$ is the normalizing constant of the Gaussian density, and similarly
\begin{align*}
    \int q_\theta(W) \log p(\bfY\vert \bfX, W)\,\D W &= \int q_\theta(W) \log \prod_{i=1}^N \frac{1}{C} e^{-\frac{\tau^{2d}}{2}\|f_W(\bfx_i)-\bfy_i\|_2^2} \,\D W \\
    &\geq -N\log(C) - \sum_{i=1}^N \frac{\tau^{2d}}{2}\|f_{\overline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i)-\bfy_i\|_2^2 \\
    &=  -N\log(C) - \sum_{i=1}^N \frac{\tau^{2d}}{2}\|f_{\underline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i) -  \bfy_i + f_{\overline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i) - f_{\underline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i)\|_2^2 \\
    &\geq -N\log(C) - \sum_{i=1}^N \tau^{2d} \left( \|f_{\underline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i)-\bfy_i\|_2^2 + \|f_{\overline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i)-f_{\underline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i)\|_2^2 \right) \\
    & \geq -N\log(C) - \sum_{i=1}^N \tau^{2d}\left(\|f_{\underline{\bfw}(\bfy_i,\bfx_i)}(\bfx_i)-\bfy_i\|_2^2 + \|\overline{f_W(\bfx_i)}-\underline{f_W(\bfx_i)}\|_2^2 \right)
\end{align*}

\subsection{Interval Inference Approximation}\label{sec:interval_inference}
If we use $q^\ast(W)$ to approximate $p(W\vert\bfX,\bfY)$ for the inference in \cref{eq:y_post}, we can again bound
\begin{align*}
 p(\bfy^\ast\vert\bfx^\ast,\bfX,\bfY) & \approx \int p(\bfy^\ast\vert\bfx^\ast,W) q^\ast(W)\,\D W \\
 &= \int q^\ast(W) \frac{1}{C} e^{-\frac{\tau^{2d}}{2}\|f_W(\bfx^\ast)-\bfy^\ast\|_2^2} \,\D W \\
 &\leq \frac{1}{C} e^{-\frac{\tau^{2d}}{2} \|f_{\underline{\bfw}(\bfy^\ast,\bfx^\ast)}(\bfx^\ast) - \bfy^\ast\|_2^2}
\end{align*}
and similarly
\begin{align*}
 p(\bfy^\ast\vert\bfx^\ast,\bfX,\bfY) & \approx \int p(\bfy^\ast\vert\bfx^\ast,W) q^\ast(W)\,\D W \\
 &\geq \frac{1}{C} e^{-\frac{\tau^{2d}}{2} \|f_{\overline{\bfw}(\bfy^\ast,\bfx^\ast)}(\bfx^\ast) - \bfy^\ast\|_2^2}.
\end{align*}

\end{document}
