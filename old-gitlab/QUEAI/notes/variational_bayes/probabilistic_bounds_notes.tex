\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage{geometry}

\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{cleveref}

\input{abbrv}

\title{Probabilistic Bounds for the Interval Method}
\author{placeholder}

%%%%% %%%%% %%%%% %%%%% %%%%%
\begin{document}

\maketitle

\section*{Loss}

For some data distribution $X,Y$ and some regularization parameter $\beta$ we use the following loss:
\begin{align*}
\mathcal{E} = \int_X \E\left[\text{max}(Y_x-\overline{y}_x,0)^2\right]+\E\left[\text{max}(\underline{y}_x-Y_x,0)^2\right]+\beta (\overline{y}_x-\underline{y}_x)\ d\P_X.
\end{align*}
Assuming during training this loss is optimized yields
\begin{align*}
0 &= \int_X \frac{\partial }{\partial \overline{y}_x} \left( \E\left[\text{max}(Y_x-\overline{y}_x,0)^2\right]+\E\left[\text{max}(\underline{y}_x-Y_x,0)^2\right]+\beta (\overline{y}_x-\underline{y}_x)\right) \ d\P_X \\
&= -\int_X 2\E\left[\text{max}(Y_x-\overline{y}_x,0)\right]\ d\P_X+\beta\\
\Rightarrow \frac{1}{2}\beta &= \int_X \E\left[\text{max}(Y_x-\overline{y}_x,0)\right]\ d\P_X.
\intertext{Analogously}
\frac{1}{2}\beta &= \int_X \E\left[\text{max}(\underline{y}_x-Y_x,0)\right]\ d\P_X.
\end{align*}

\section*{Using the Markow Inequality}
Using the Markow Inequality with $h_1(\zeta):= \text{max}(\zeta-\overline{y}_x,0)$ and $h_2(\zeta):= \text{max}(\zeta+\underline{y}_x,0)$ we can see that for the marginalized distribution $Y_x$ the following holds:
\begin{align*}
&\P(Y_x \geq \overline{y}_x+\lambda \beta)  \leq \frac{\E\left[h_1(Y_x) \right]}{h_1(\overline{y}_x+\lambda \beta)} = \frac{\E\left[\text{max}(Y_x-\overline{y}_x,0) \right]}{\lambda \beta}
\intertext{and}
&\P(Y_x \leq \underline{y}_x-\lambda \beta)\\ = &\P(-Y_x \geq -\underline{y}_x+\lambda \beta) \leq \frac{\E\left[h_2(-Y_x) \right]}{h_2(-\underline{y}_x+\lambda \beta)} = \frac{\E\left[\text{max}(\underline{y}_x-Y_x,0) \right]}{\lambda \beta}.
\end{align*}
Using the previous equation we can now see the following:
\begin{align*}
&\P(\left\lbrace\text{Label is inside interval bounds plus $\lambda \beta$} \right\rbrace)\\=&\int_X \P(\underline{y}_x-\lambda \beta\leq Y_x \leq \overline{y}_x+\lambda \beta)\ d\P_X
= 1-\int_X \P(Y_x \leq \underline{y}_x-\lambda \beta)+ \P(Y_x \geq \overline{y}_x+\lambda \beta)\ d\P_X
\end{align*}
with
\begin{align*}
\int_X \P(Y_x \leq \underline{y}_x-\lambda \beta)+ \P(Y_x \geq \overline{y}_x+\lambda \beta)\ d\P_X &\leq \int_X \frac{\E\left[\text{max}(Y_x-\overline{y}_x,0) \right]}{\lambda \beta}+\frac{\E\left[\text{max}(\underline{y}_x-Y_x,0) \right]}{\lambda \beta} d\P_X\\
&= \frac{1}{\lambda}.
\end{align*}
Therefore
\begin{align*}
&\P(\left\lbrace\text{Label is inside interval bounds plus $\lambda \beta$} \right\rbrace) \geq 1-\frac{1}{\lambda}.
\end{align*}
We can furthermore bound the probability that for a given data point $x$ the label $y_x$ has a probability of more than $\alpha$ to be outside the interval bounds:
\begin{align*}
\frac{1}{\lambda} &\geq \E_X\left[\P( Y_x < \underline{y}_x-\lambda \beta,\ Y_x > \overline{y}_x+\lambda \beta) \right] \geq \E_X\left[\alpha \1_{\P( Y_x < \underline{y}_x-\lambda \beta,\ Y_x > \overline{y}_x+\lambda \beta)> \alpha} \right]\\
\Rightarrow \frac{1}{\lambda \alpha} &\geq \E_X\left[\1_{\P( Y_x < \underline{y}_x-\lambda \beta,\ Y_x > \overline{y}_x+\lambda \beta)> \alpha} \right] = \E_X\left[1-\1_{\P( Y_x < \underline{y}_x-\lambda \beta,\ Y_x > \overline{y}_x+\lambda \beta)leq \alpha} \right]\\
&= 1-\E_X\left[\1_{\P(\underline{y}_x-\lambda \beta\leq Y_x \leq \overline{y}_x+\lambda \beta)\geq 1-\alpha} \right]\\
\Rightarrow 1-\frac{1}{\lambda \alpha} &\leq \E_X\left[\1_{\P(\underline{y}_x-\lambda \beta\leq Y_x \leq \overline{y}_x+\lambda \beta)\geq 1-\alpha} \right]
\end{align*}
In other words, for $\lambda>0$ and $\alpha>0$ the probability mass of all samples $x$ where corresponding label $y_x$ has the probability of at least $1-\alpha$ to be inside the interval is at least $ 1-\frac{1}{\lambda \alpha}$.


\end{document}
